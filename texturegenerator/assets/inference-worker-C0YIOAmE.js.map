{"version":3,"file":"inference-worker-C0YIOAmE.js","sources":["../src/types/inference.ts","../src/inference/worker/inference-worker.ts"],"sourcesContent":["/**\n * Type definitions for AI inference system.\n * Covers backend capabilities, model management, and worker communication.\n */\n\n/**\n * Browser capability detection results\n */\nexport interface BackendCapabilities {\n  /** WebGPU API available */\n  webgpu: boolean;\n  /** WebGPU adapter reference (null if unavailable) */\n  webgpuAdapter: GPUAdapter | null;\n  /** FP16 shader support available */\n  webgpuFP16: boolean;\n  /** WebNN API available */\n  webnn: boolean;\n  /** WASM available (always true in modern browsers) */\n  wasm: boolean;\n  /** WASM SIMD support */\n  wasmSimd: boolean;\n  /** WASM threads support (SharedArrayBuffer) */\n  wasmThreads: boolean;\n  /** Estimated available memory in MB */\n  estimatedMemoryMB: number;\n  /** WebGL2 available */\n  webgl2: boolean;\n}\n\n/**\n * Available inference backends in priority order\n */\nexport type InferenceBackend = 'webgpu-fp16' | 'webgpu-fp32' | 'webnn' | 'wasm';\n\n/**\n * Model quantization types\n */\nexport type ModelQuantization = 'fp32' | 'fp16' | 'int8' | 'int4';\n\n/**\n * Model descriptor from manifest\n */\nexport interface ModelDescriptor {\n  /** Unique model identifier */\n  id: string;\n  /** Display name */\n  name: string;\n  /** Model file path (relative to models directory) */\n  path: string;\n  /** External data file path (for models >2GB) */\n  externalDataPath?: string;\n  /** Model size in bytes */\n  sizeBytes: number;\n  /** Quantization type */\n  quantization: ModelQuantization;\n  /** Compatible backends */\n  backends: InferenceBackend[];\n  /** Input tensor shape [batch, channels, height, width] */\n  inputShape: [number, number, number, number];\n  /** Output channels (e.g., 3 for normal map RGB) */\n  outputChannels: number;\n  /** Required memory for inference (MB) */\n  requiredMemoryMB: number;\n}\n\n/**\n * Model manifest file structure\n */\nexport interface ModelManifest {\n  version: string;\n  models: ModelDescriptor[];\n}\n\n/**\n * Model cache entry metadata\n */\nexport interface ModelCacheEntry {\n  /** Model ID */\n  modelId: string;\n  /** Cache timestamp */\n  cachedAt: number;\n  /** Model file size */\n  sizeBytes: number;\n  /** Cache version for invalidation */\n  cacheVersion: string;\n}\n\n/**\n * Download progress information\n */\nexport interface DownloadProgress {\n  /** Model being downloaded */\n  modelId: string;\n  /** Bytes downloaded */\n  loadedBytes: number;\n  /** Total bytes */\n  totalBytes: number;\n  /** Progress percentage 0-1 */\n  progress: number;\n}\n\n/**\n * Worker message types\n */\nexport type WorkerMessageType =\n  | 'init'\n  | 'ready'\n  | 'error'\n  | 'infer'\n  | 'result'\n  | 'progress'\n  | 'dispose';\n\n/**\n * Base worker message structure\n */\nexport interface WorkerMessageBase {\n  type: WorkerMessageType;\n  id?: number;\n}\n\n/**\n * Initialize worker message\n */\nexport interface WorkerInitMessage extends WorkerMessageBase {\n  type: 'init';\n  payload: {\n    modelUrl: string;\n    externalDataUrl?: string;\n    backend: InferenceBackend;\n  };\n}\n\n/**\n * Worker ready message\n */\nexport interface WorkerReadyMessage extends WorkerMessageBase {\n  type: 'ready';\n}\n\n/**\n * Worker error message\n */\nexport interface WorkerErrorMessage extends WorkerMessageBase {\n  type: 'error';\n  payload: {\n    code: string;\n    message: string;\n  };\n}\n\n/**\n * Inference request message\n */\nexport interface WorkerInferMessage extends WorkerMessageBase {\n  type: 'infer';\n  id: number;\n  payload: {\n    imageData: ArrayBuffer;\n    width: number;\n    height: number;\n  };\n}\n\n/**\n * Inference result message\n */\nexport interface WorkerResultMessage extends WorkerMessageBase {\n  type: 'result';\n  id: number;\n  payload: {\n    outputData: ArrayBuffer;\n    width: number;\n    height: number;\n  };\n}\n\n/**\n * Inference progress message\n */\nexport interface WorkerProgressMessage extends WorkerMessageBase {\n  type: 'progress';\n  id: number;\n  payload: {\n    stage: string;\n    progress: number;\n  };\n}\n\n/**\n * Dispose worker message\n */\nexport interface WorkerDisposeMessage extends WorkerMessageBase {\n  type: 'dispose';\n}\n\n/**\n * Union of all worker messages\n */\nexport type WorkerMessage =\n  | WorkerInitMessage\n  | WorkerReadyMessage\n  | WorkerErrorMessage\n  | WorkerInferMessage\n  | WorkerResultMessage\n  | WorkerProgressMessage\n  | WorkerDisposeMessage;\n\n/**\n * Inference session state\n */\nexport type SessionState = 'uninitialized' | 'loading' | 'ready' | 'busy' | 'error' | 'disposed';\n\n/**\n * Error codes for inference operations\n */\nexport enum InferenceErrorCode {\n  BACKEND_NOT_SUPPORTED = 'BACKEND_NOT_SUPPORTED',\n  MODEL_NOT_FOUND = 'MODEL_NOT_FOUND',\n  MODEL_LOAD_FAILED = 'MODEL_LOAD_FAILED',\n  INFERENCE_FAILED = 'INFERENCE_FAILED',\n  OUT_OF_MEMORY = 'OUT_OF_MEMORY',\n  WORKER_ERROR = 'WORKER_ERROR',\n  SESSION_NOT_READY = 'SESSION_NOT_READY',\n  CACHE_ERROR = 'CACHE_ERROR',\n}\n\n/**\n * Inference error with recovery information\n */\nexport class InferenceError extends Error {\n  constructor(\n    message: string,\n    public readonly code: InferenceErrorCode,\n    public readonly recoverable: boolean = false,\n    public readonly suggestion?: string\n  ) {\n    super(message);\n    this.name = 'InferenceError';\n  }\n}\n","/**\n * Web Worker for AI inference.\n * Runs ONNX Runtime inference in a separate thread to avoid blocking the UI.\n *\n * Note: This file is compiled as a separate entry point and loaded as a Worker.\n */\n\nimport type {\n  WorkerMessage,\n  WorkerInitMessage,\n  WorkerInferMessage,\n  InferenceBackend,\n} from '../../types/inference.ts';\nimport { InferenceErrorCode } from '../../types/inference.ts';\n\n// Declare self as DedicatedWorkerGlobalScope\ndeclare const self: DedicatedWorkerGlobalScope;\n\n// ONNX Runtime will be dynamically imported based on backend\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nlet ort: any = null;\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nlet session: any = null;\n\n/**\n * Send a message to the main thread.\n */\nfunction sendMessage(message: WorkerMessage): void {\n  self.postMessage(message);\n}\n\n/**\n * Send a transferable message (with ArrayBuffer transfer).\n */\nfunction sendTransferable(message: WorkerMessage, transfers: Transferable[]): void {\n  (self as DedicatedWorkerGlobalScope).postMessage(message, transfers);\n}\n\n/**\n * Send an error message.\n */\nfunction sendError(code: string, message: string, id?: number): void {\n  sendMessage({\n    type: 'error',\n    id,\n    payload: { code, message },\n  });\n}\n\n/**\n * Send progress update.\n */\nfunction sendProgress(id: number, stage: string, progress: number): void {\n  sendMessage({\n    type: 'progress',\n    id,\n    payload: { stage, progress },\n  });\n}\n\n/**\n * Initialize the ONNX Runtime session.\n */\nasync function initSession(payload: WorkerInitMessage['payload']): Promise<void> {\n  const { modelUrl, externalDataUrl, backend } = payload;\n\n  try {\n    // Dynamic import based on backend\n    if (backend.startsWith('webgpu')) {\n      // Import WebGPU-enabled ONNX Runtime\n      // Note: In production, this would be: import * as ort from 'onnxruntime-web/webgpu';\n      // For now, we use the base package which supports multiple backends\n      ort = await import('onnxruntime-web');\n    } else {\n      ort = await import('onnxruntime-web');\n    }\n\n    // Configure execution providers\n    const executionProviders = getExecutionProviders(backend);\n\n    // Session options\n    const sessionOptions: Record<string, unknown> = {\n      executionProviders,\n      graphOptimizationLevel: 'all',\n    };\n\n    // Add external data if provided\n    if (externalDataUrl) {\n      sessionOptions.externalData = [\n        {\n          path: externalDataUrl.split('/').pop(),\n          data: externalDataUrl,\n        },\n      ];\n    }\n\n    // Fetch model as ArrayBuffer for more reliable loading\n    const response = await fetch(modelUrl);\n    if (!response.ok) {\n      throw new Error(`Failed to fetch model: HTTP ${response.status}`);\n    }\n    const modelData = await response.arrayBuffer();\n\n    // Try WASM backend first for compatibility (most ops supported)\n    const wasmFirst: Record<string, unknown> = {\n      executionProviders: ['wasm'],\n      graphOptimizationLevel: 'basic',\n    };\n\n    try {\n      session = await ort.InferenceSession.create(modelData, wasmFirst);\n    } catch {\n      // Try original settings if WASM fails\n      session = await ort.InferenceSession.create(modelData, sessionOptions);\n    }\n\n    sendMessage({ type: 'ready' });\n  } catch (e) {\n    const message = e instanceof Error ? e.message : 'Unknown error';\n    console.error('Session creation failed:', e);\n    sendError(InferenceErrorCode.MODEL_LOAD_FAILED, `Failed to load model: ${message}`);\n  }\n}\n\n/**\n * Get ONNX Runtime execution providers for a backend.\n */\nfunction getExecutionProviders(backend: InferenceBackend): string[] {\n  switch (backend) {\n    case 'webgpu-fp16':\n    case 'webgpu-fp32':\n      return ['webgpu', 'wasm'];\n    case 'webnn':\n      return ['webnn', 'wasm'];\n    case 'wasm':\n      return ['wasm'];\n    default:\n      return ['wasm'];\n  }\n}\n\n/**\n * Run inference on input image data.\n */\nasync function runInference(id: number, payload: WorkerInferMessage['payload']): Promise<void> {\n  if (!session || !ort) {\n    sendError(InferenceErrorCode.SESSION_NOT_READY, 'Session not initialized', id);\n    return;\n  }\n\n  const { imageData, width, height } = payload;\n\n  try {\n    sendProgress(id, 'preprocessing', 0.1);\n\n    // Convert RGBA ArrayBuffer to Float32 NCHW tensor\n    const inputTensor = preprocessImage(new Uint8ClampedArray(imageData), width, height);\n\n    sendProgress(id, 'inference', 0.2);\n\n    // Get input/output names from session\n    const inputName = session.inputNames[0];\n    const outputName = session.outputNames[0];\n\n    // Run inference\n    const feeds: Record<string, unknown> = {};\n    feeds[inputName] = inputTensor;\n\n    const results = await session.run(feeds);\n\n    sendProgress(id, 'postprocessing', 0.9);\n\n    // Get output tensor\n    const outputTensor = results[outputName];\n    const outputData = postprocessOutput(outputTensor, width, height);\n\n    sendProgress(id, 'complete', 1.0);\n\n    // Send result with transfer - explicitly create ArrayBuffer\n    const transferBuffer = outputData.buffer.slice(0) as ArrayBuffer;\n    sendTransferable(\n      {\n        type: 'result',\n        id,\n        payload: {\n          outputData: transferBuffer,\n          width,\n          height,\n        },\n      },\n      [transferBuffer]\n    );\n  } catch (e) {\n    const message = e instanceof Error ? e.message : 'Unknown error';\n\n    // Check for OOM\n    if (message.includes('memory') || message.includes('OOM')) {\n      sendError(InferenceErrorCode.OUT_OF_MEMORY, 'Out of memory during inference', id);\n    } else {\n      sendError(InferenceErrorCode.INFERENCE_FAILED, `Inference failed: ${message}`, id);\n    }\n  }\n}\n\n/**\n * Convert RGBA ImageData to Float32 NCHW tensor.\n * DeepBump expects grayscale input [1, 1, 256, 256]\n */\nfunction preprocessImage(\n  data: Uint8ClampedArray,\n  width: number,\n  height: number\n): unknown {\n  const pixelCount = width * height;\n\n  // DeepBump expects grayscale input (1 channel)\n  const float32Data = new Float32Array(pixelCount);\n\n  // Convert RGB to grayscale using luminance formula, normalized to [0,1]\n  for (let i = 0; i < pixelCount; i++) {\n    const r = data[i * 4] / 255;\n    const g = data[i * 4 + 1] / 255;\n    const b = data[i * 4 + 2] / 255;\n    // Luminance formula\n    float32Data[i] = 0.299 * r + 0.587 * g + 0.114 * b;\n  }\n\n  return new ort.Tensor('float32', float32Data, [1, 1, height, width]);\n}\n\n/**\n * Convert output tensor to RGBA ImageData buffer.\n * Flips Y to match canvas coordinate system (Y=0 at top).\n */\nfunction postprocessOutput(\n  tensor: { data: Float32Array; dims: number[] },\n  width: number,\n  height: number\n): Uint8ClampedArray {\n  const outputData = tensor.data;\n  const outputChannels = tensor.dims[1] || 3;\n  const pixelCount = width * height;\n  const result = new Uint8ClampedArray(pixelCount * 4);\n\n  if (outputChannels >= 3) {\n    // RGB or RGBA output with Y-flip\n    for (let y = 0; y < height; y++) {\n      const srcY = height - 1 - y;  // Flip Y\n      for (let x = 0; x < width; x++) {\n        const srcIdx = srcY * width + x;\n        const dstIdx = y * width + x;\n        result[dstIdx * 4] = Math.round(clamp(outputData[srcIdx], 0, 1) * 255);                      // R\n        result[dstIdx * 4 + 1] = Math.round(clamp(outputData[pixelCount + srcIdx], 0, 1) * 255);     // G\n        result[dstIdx * 4 + 2] = Math.round(clamp(outputData[2 * pixelCount + srcIdx], 0, 1) * 255); // B\n        result[dstIdx * 4 + 3] = 255;                                                                 // A\n      }\n    }\n  } else {\n    // Grayscale output with Y-flip\n    for (let y = 0; y < height; y++) {\n      const srcY = height - 1 - y;  // Flip Y\n      for (let x = 0; x < width; x++) {\n        const srcIdx = srcY * width + x;\n        const dstIdx = y * width + x;\n        const value = Math.round(clamp(outputData[srcIdx], 0, 1) * 255);\n        result[dstIdx * 4] = value;\n        result[dstIdx * 4 + 1] = value;\n        result[dstIdx * 4 + 2] = value;\n        result[dstIdx * 4 + 3] = 255;\n      }\n    }\n  }\n\n  return result;\n}\n\n/**\n * Clamp value to range.\n */\nfunction clamp(value: number, min: number, max: number): number {\n  return Math.max(min, Math.min(max, value));\n}\n\n/**\n * Dispose the session and free resources.\n */\nasync function disposeSession(): Promise<void> {\n  if (session) {\n    await session.release();\n    session = null;\n  }\n}\n\n/**\n * Message handler.\n */\nself.onmessage = async (event: MessageEvent<WorkerMessage>) => {\n  const message = event.data;\n\n  switch (message.type) {\n    case 'init':\n      await initSession((message as WorkerInitMessage).payload);\n      break;\n\n    case 'infer': {\n      const inferMsg = message as WorkerInferMessage;\n      await runInference(inferMsg.id, inferMsg.payload);\n      break;\n    }\n\n    case 'dispose':\n      await disposeSession();\n      break;\n\n    default:\n      sendError(InferenceErrorCode.WORKER_ERROR, `Unknown message type: ${message.type}`);\n  }\n};\n\n// Worker loaded and ready to receive messages\n"],"names":["InferenceErrorCode","ort","session","sendMessage","message","self","postMessage","sendError","code","id","type","payload","sendProgress","stage","progress","async","runInference","SESSION_NOT_READY","imageData","width","height","inputTensor","data","pixelCount","float32Data","Float32Array","i","r","g","b","Tensor","preprocessImage","Uint8ClampedArray","inputName","inputNames","outputName","outputNames","feeds","results","run","outputData","tensor","outputChannels","dims","result","y","srcY","x","srcIdx","dstIdx","Math","round","clamp","value","postprocessOutput","transferBuffer","buffer","slice","transfers","e","Error","includes","OUT_OF_MEMORY","INFERENCE_FAILED","min","max","onmessage","event","modelUrl","externalDataUrl","backend","startsWith","import","executionProviders","getExecutionProviders","sessionOptions","graphOptimizationLevel","externalData","path","split","pop","response","fetch","ok","status","modelData","arrayBuffer","wasmFirst","InferenceSession","create","console","error","MODEL_LOAD_FAILED","initSession","inferMsg","release","disposeSession","WORKER_ERROR"],"mappings":"AAwNO,IAAKA,GAAAA,IACVA,EAAA,sBAAwB,wBACxBA,EAAA,gBAAkB,kBAClBA,EAAA,kBAAoB,oBACpBA,EAAA,iBAAmB,mBACnBA,EAAA,cAAgB,gBAChBA,EAAA,aAAe,eACfA,EAAA,kBAAoB,oBACpBA,EAAA,YAAc,cARJA,IAAAA,GAAA,CAAA,GCpMZ,IAAIC,EAAW,KAEXC,EAAe,KAKnB,SAASC,EAAYC,GACnBC,KAAKC,YAAYF,EACnB,CAYA,SAASG,EAAUC,EAAcJ,EAAiBK,GAChDN,EAAY,CACVO,KAAM,QACND,KACAE,QAAS,CAAEH,OAAMJ,YAErB,CAKA,SAASQ,EAAaH,EAAYI,EAAeC,GAC/CX,EAAY,CACVO,KAAM,WACND,KACAE,QAAS,CAAEE,QAAOC,aAEtB,CAsFAC,eAAeC,EAAaP,EAAYE,GACtC,IAAKT,IAAYD,EAEf,YADAM,EAAUP,EAAmBiB,kBAAmB,0BAA2BR,GAI7E,MAAMS,UAAEA,EAAAC,MAAWA,EAAAC,OAAOA,GAAWT,EAErC,IACEC,EAAaH,EAAI,gBAAiB,IAGlC,MAAMY,EAoDV,SACEC,EACAH,EACAC,GAEA,MAAMG,EAAaJ,EAAQC,EAGrBI,EAAc,IAAIC,aAAaF,GAGrC,IAAA,IAASG,EAAI,EAAGA,EAAIH,EAAYG,IAAK,CACnC,MAAMC,EAAIL,EAAS,EAAJI,GAAS,IAClBE,EAAIN,EAAS,EAAJI,EAAQ,GAAK,IACtBG,EAAIP,EAAS,EAAJI,EAAQ,GAAK,IAE5BF,EAAYE,GAAK,KAAQC,EAAI,KAAQC,EAAI,KAAQC,CACnD,CAEA,OAAO,IAAI5B,EAAI6B,OAAO,UAAWN,EAAa,CAAC,EAAG,EAAGJ,EAAQD,GAC/D,CAxEwBY,CAAgB,IAAIC,kBAAkBd,GAAYC,EAAOC,GAE7ER,EAAaH,EAAI,YAAa,IAG9B,MAAMwB,EAAY/B,EAAQgC,WAAW,GAC/BC,EAAajC,EAAQkC,YAAY,GAGjCC,EAAiC,CAAA,EACvCA,EAAMJ,GAAaZ,EAEnB,MAAMiB,QAAgBpC,EAAQqC,IAAIF,GAElCzB,EAAaH,EAAI,iBAAkB,IAGnC,MACM+B,EA4DV,SACEC,EACAtB,EACAC,GAEA,MAAMoB,EAAaC,EAAOnB,KACpBoB,EAAiBD,EAAOE,KAAK,IAAM,EACnCpB,EAAaJ,EAAQC,EACrBwB,EAAS,IAAIZ,kBAA+B,EAAbT,GAErC,GAAImB,GAAkB,EAEpB,IAAA,IAASG,EAAI,EAAGA,EAAIzB,EAAQyB,IAAK,CAC/B,MAAMC,EAAO1B,EAAS,EAAIyB,EAC1B,IAAA,IAASE,EAAI,EAAGA,EAAI5B,EAAO4B,IAAK,CAC9B,MAAMC,EAASF,EAAO3B,EAAQ4B,EACxBE,EAASJ,EAAI1B,EAAQ4B,EAC3BH,EAAgB,EAATK,GAAcC,KAAKC,MAAwC,IAAlCC,EAAMZ,EAAWQ,GAAS,EAAG,IAC7DJ,EAAgB,EAATK,EAAa,GAAKC,KAAKC,MAAqD,IAA/CC,EAAMZ,EAAWjB,EAAayB,GAAS,EAAG,IAC9EJ,EAAgB,EAATK,EAAa,GAAKC,KAAKC,MAAyD,IAAnDC,EAAMZ,EAAW,EAAIjB,EAAayB,GAAS,EAAG,IAClFJ,EAAgB,EAATK,EAAa,GAAK,GAC3B,CACF,MAGA,IAAA,IAASJ,EAAI,EAAGA,EAAIzB,EAAQyB,IAAK,CAC/B,MAAMC,EAAO1B,EAAS,EAAIyB,EAC1B,IAAA,IAASE,EAAI,EAAGA,EAAI5B,EAAO4B,IAAK,CAC9B,MAAMC,EAASF,EAAO3B,EAAQ4B,EACxBE,EAASJ,EAAI1B,EAAQ4B,EACrBM,EAAQH,KAAKC,MAAwC,IAAlCC,EAAMZ,EAAWQ,GAAS,EAAG,IACtDJ,EAAgB,EAATK,GAAcI,EACrBT,EAAgB,EAATK,EAAa,GAAKI,EACzBT,EAAgB,EAATK,EAAa,GAAKI,EACzBT,EAAgB,EAATK,EAAa,GAAK,GAC3B,CACF,CAGF,OAAOL,CACT,CApGuBU,CADEhB,EAAQH,GACsBhB,EAAOC,GAE1DR,EAAaH,EAAI,WAAY,GAG7B,MAAM8C,EAAiBf,EAAWgB,OAAOC,MAAM,GAjJzBrD,EAmJpB,CACEM,KAAM,SACND,KACAE,QAAS,CACP6B,WAAYe,EACZpC,QACAC,WAzJwCsC,EA4J5C,CAACH,GA3JJlD,KAAoCC,YAAYF,EAASsD,EA6J1D,OAASC,GACP,MAAMvD,EAAUuD,aAAaC,MAAQD,EAAEvD,QAAU,gBAG7CA,EAAQyD,SAAS,WAAazD,EAAQyD,SAAS,OACjDtD,EAAUP,EAAmB8D,cAAe,iCAAkCrD,GAE9EF,EAAUP,EAAmB+D,iBAAkB,qBAAqB3D,IAAWK,EAEnF,CAvKF,IAA0BL,EAAwBsD,CAwKlD,CA6EA,SAASN,EAAMC,EAAeW,EAAaC,GACzC,OAAOf,KAAKe,IAAID,EAAKd,KAAKc,IAAIC,EAAKZ,GACrC,CAeAhD,KAAK6D,UAAYnD,MAAOoD,IACtB,MAAM/D,EAAU+D,EAAM7C,KAEtB,OAAQlB,EAAQM,MACd,IAAK,aA7OTK,eAA2BJ,GACzB,MAAMyD,SAAEA,EAAAC,gBAAUA,EAAAC,QAAiBA,GAAY3D,EAE/C,IAEM2D,EAAQC,WAAW,UAIrBtE,QAAYuE,OAAO,gCAMrB,MAAMC,EAiDV,SAA+BH,GAC7B,OAAQA,GACN,IAAK,cACL,IAAK,cACH,MAAO,CAAC,SAAU,QACpB,IAAK,QACH,MAAO,CAAC,QAAS,QAGnB,QACE,MAAO,CAAC,QAEd,CA7D+BI,CAAsBJ,GAG3CK,EAA0C,CAC9CF,qBACAG,uBAAwB,OAItBP,IACFM,EAAeE,aAAe,CAC5B,CACEC,KAAMT,EAAgBU,MAAM,KAAKC,MACjC1D,KAAM+C,KAMZ,MAAMY,QAAiBC,MAAMd,GAC7B,IAAKa,EAASE,GACZ,MAAM,IAAIvB,MAAM,+BAA+BqB,EAASG,UAE1D,MAAMC,QAAkBJ,EAASK,cAG3BC,EAAqC,CACzCd,mBAAoB,CAAC,QACrBG,uBAAwB,SAG1B,IACE1E,QAAgBD,EAAIuF,iBAAiBC,OAAOJ,EAAWE,EACzD,CAAA,MAEErF,QAAgBD,EAAIuF,iBAAiBC,OAAOJ,EAAWV,EACzD,CAEAxE,EAAY,CAAEO,KAAM,SACtB,OAASiD,GACP,MAAMvD,EAAUuD,aAAaC,MAAQD,EAAEvD,QAAU,gBACjDsF,QAAQC,MAAM,2BAA4BhC,GAC1CpD,EAAUP,EAAmB4F,kBAAmB,yBAAyBxF,IAC3E,CACF,CAmLYyF,CAAazF,EAA8BO,SACjD,MAEF,IAAK,QAAS,CACZ,MAAMmF,EAAW1F,QACXY,EAAa8E,EAASrF,GAAIqF,EAASnF,SACzC,KACF,CAEA,IAAK,gBAxBTI,iBACMb,UACIA,EAAQ6F,UACd7F,EAAU,KAEd,CAoBY8F,GACN,MAEF,QACEzF,EAAUP,EAAmBiG,aAAc,yBAAyB7F,EAAQM"}