var e=(e=>(e.BACKEND_NOT_SUPPORTED="BACKEND_NOT_SUPPORTED",e.MODEL_NOT_FOUND="MODEL_NOT_FOUND",e.MODEL_LOAD_FAILED="MODEL_LOAD_FAILED",e.INFERENCE_FAILED="INFERENCE_FAILED",e.OUT_OF_MEMORY="OUT_OF_MEMORY",e.WORKER_ERROR="WORKER_ERROR",e.SESSION_NOT_READY="SESSION_NOT_READY",e.CACHE_ERROR="CACHE_ERROR",e))(e||{});let t=null,n=null;function a(e){self.postMessage(e)}function o(e,t,n){a({type:"error",id:n,payload:{code:e,message:t}})}function r(e,t,n){a({type:"progress",id:e,payload:{stage:t,progress:n}})}async function s(a,s){if(!n||!t)return void o(e.SESSION_NOT_READY,"Session not initialized",a);const{imageData:c,width:l,height:u}=s;try{r(a,"preprocessing",.1);const e=function(e,n,a){const o=n*a,r=new Float32Array(o);for(let t=0;t<o;t++){const n=e[4*t]/255,a=e[4*t+1]/255,o=e[4*t+2]/255;r[t]=.299*n+.587*a+.114*o}return new t.Tensor("float32",r,[1,1,a,n])}(new Uint8ClampedArray(c),l,u);r(a,"inference",.2);const o=n.inputNames[0],s=n.outputNames[0],d={};d[o]=e;const f=await n.run(d);r(a,"postprocessing",.9);const p=function(e,t,n){const a=e.data,o=e.dims[1]||3,r=t*n,s=new Uint8ClampedArray(4*r);if(o>=3)for(let c=0;c<n;c++){const e=n-1-c;for(let n=0;n<t;n++){const o=e*t+n,l=c*t+n;s[4*l]=Math.round(255*i(a[o],0,1)),s[4*l+1]=Math.round(255*i(a[r+o],0,1)),s[4*l+2]=Math.round(255*i(a[2*r+o],0,1)),s[4*l+3]=255}}else for(let c=0;c<n;c++){const e=n-1-c;for(let n=0;n<t;n++){const o=e*t+n,r=c*t+n,l=Math.round(255*i(a[o],0,1));s[4*r]=l,s[4*r+1]=l,s[4*r+2]=l,s[4*r+3]=255}}return s}(f[s],l,u);r(a,"complete",1);const _=p.buffer.slice(0);E={type:"result",id:a,payload:{outputData:_,width:l,height:u}},O=[_],self.postMessage(E,O)}catch(d){const t=d instanceof Error?d.message:"Unknown error";t.includes("memory")||t.includes("OOM")?o(e.OUT_OF_MEMORY,"Out of memory during inference",a):o(e.INFERENCE_FAILED,`Inference failed: ${t}`,a)}var E,O}function i(e,t,n){return Math.max(t,Math.min(n,e))}self.onmessage=async r=>{const i=r.data;switch(i.type){case"init":await async function(r){const{modelUrl:s,externalDataUrl:i,backend:c}=r;try{c.startsWith("webgpu"),t=await import("./ort.bundle.min-DIOOwSkD.js");const e=function(e){switch(e){case"webgpu-fp16":case"webgpu-fp32":return["webgpu","wasm"];case"webnn":return["webnn","wasm"];default:return["wasm"]}}(c),o={executionProviders:e,graphOptimizationLevel:"all"};i&&(o.externalData=[{path:i.split("/").pop(),data:i}]);const r=await fetch(s);if(!r.ok)throw new Error(`Failed to fetch model: HTTP ${r.status}`);const l=await r.arrayBuffer(),u={executionProviders:["wasm"],graphOptimizationLevel:"basic"};try{n=await t.InferenceSession.create(l,u)}catch{n=await t.InferenceSession.create(l,o)}a({type:"ready"})}catch(l){const t=l instanceof Error?l.message:"Unknown error";console.error("Session creation failed:",l),o(e.MODEL_LOAD_FAILED,`Failed to load model: ${t}`)}}(i.payload);break;case"infer":{const e=i;await s(e.id,e.payload);break}case"dispose":await async function(){n&&(await n.release(),n=null)}();break;default:o(e.WORKER_ERROR,`Unknown message type: ${i.type}`)}};
